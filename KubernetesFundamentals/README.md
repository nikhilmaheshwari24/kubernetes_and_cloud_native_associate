## KubernetesFundamentals

Hello, and welcome to this lecture on Kubernetes overview. In this lecture, we will go through an overview of Kubernetes. Kubernetes also known as K8s was built by Google based on their experience running containers in production. It is now an open source project and is arguably one of the best and most popular container orchestration technologies out there. In this lecture, we will try to understand Kubernetes at a high level. To understand Kubernetes, we must first understand two things, containers and orchestration. Once we get familiarized with both of these terms, we would be in a position to understand what Kubernetes is capable of. We will start by looking at each of these next. We're now going to look at what containers are. Specifically, we will look at the most popular container technology out there. That is **Docker**. 

If you are familiar with Docker already, feel free to skip this lecture and move over to the next. Let me start by sharing how I got introduced to Docker. In one of my previous projects, I had this requirement to set up an end-to-end stack including various different technologies like a web server using Node.js, a database such as MongoDB, a messaging system like Redis, and an orchestration tool like Ansible. We had a lot of issues developing this application with all these different components. First, their compatibility with the underlying operating system. We had to ensure that all these different services were compatible with the version of the operating system we were planning to use. There have been times when certain versions of these services were not compatible with the OS, and we've had to go back and look for another OS that was compatible with all these different services. Secondly, we had to check the compatibility between these services and the libraries and dependencies on the OS. We've had issues where one service requires one version of a dependent library, whereas another service requires another version. The architecture of our application changed over time. We've had to upgrade to newer versions of these components or change the database, etc. And every time something changed, we had to go through the same process of checking compatibility between these various components and the underlying infrastructure. This compatibility matrix issue is usually referred to as the matrix from hell. 

Next, every time we had a new developer on board, we found it really difficult to set up a new environment. The new developers had to follow a large set of instructions and run hundreds of commands to finally set up their environments. They had to make sure they were using the right operating system, the right versions of each of these components, and each developer had to set all that up by himself each time. We also had different development, test, and production environments. One developer may be comfortable using one OS and the others may be using another one. And so we couldn't guarantee that the application that we were building would run the same way in different environments. And so all of this made our life in developing, building, and shipping the application really difficult. 

So I needed something that could help us with the compatibility issue, something that would allow us to modify or change these components without affecting the other components and even modify the underlying operating system as required. And that search landed me on Docker. With Docker, I was able to run each component in a separate container with its own libraries and its own dependencies, all on the same VM and the OS, but within separate environments or containers. We just had to build the Docker configuration once and all our developers could now get started with a simple Docker run command. Irrespective of what the underlying operating system they run, all they needed to do was to make sure they had Docker installed on their systems. 

So what are containers? Containers are completely isolated environments, as in they can have their own processes or services, their own networking interfaces, their own mounts, just like virtual machines, except they all share the same operating system kernel. We will look at what that means in a bit. But it's also important to note that containers are not new with Docker. Containers have existed for about years now, and some of the different types of containers are **LXC**, **LXD**, **LXCFS**, etc. Docker utilizes **LXC** containers. Setting up these container environments is hard as they are very low level, and that is where Docker offers a high-level tool with several powerful functions, making it really easy for end users like us. 

To understand how Docker works, let us revisit some basic concepts of operating systems first. If you look at operating systems like Ubuntu, Fedora, SUSE, or CentOS, they all consist of two things, an OS kernel and a set of software. The operating system kernel is responsible for interacting with the underlying hardware, while the OS kernel remains the same, which is Linux in this case. It's the software above it that makes these operating systems different. This software may consist of a different user interface, drivers, compilers, file managers, developer tools, etc. So you have a common Linux kernel shared across all operating systems, and some custom software that differentiates operating systems from each other. We said earlier that Docker containers share the underlying kernel. What does that actually mean, sharing the kernel? Let's say we have a system with an Ubuntu OS with Docker installed on it. Docker can run any flavor of OS on top of it, as long as they are all based on the same kernel, in this case Linux. If the underlying operating system is Ubuntu, Docker can run a container based on another distribution like Debian, Fedora, SUSE, or CentOS. Each Docker container only has the additional software that we just talked about in the previous slide that makes these operating systems different, and Docker utilizes the underlying kernel of the Docker host, which works with all the operating systems above. So what is an OS that does not share the same kernel as this? Windows. And so you won't be able to run a Windows-based container on a Docker host with Linux OS on it. For that, you would require Docker on a Windows Server. You might ask, isn't that a disadvantage then? Not being able to run another kernel on the OS. The answer is no, because unlike **hypervisors**, Docker is not meant to virtualize and run different operating systems and kernels on the same hardware. The main purpose of Docker is to containerize applications and to ship them and run them. 

So that brings us to the differences between virtual machines and containers, something that we tend to do, especially those from a virtualization background. As you can see on the right, in case of Docker, we have the underlying hardware infrastructure, then the operating system, and Docker installed on the OS. Docker can then manage the containers that run with libraries and dependencies alone. In case of a virtual machine, we have the OS on the underlying hardware, then the hypervisor like ESXi or virtualization of some kind, and then the virtual machines. As you can see, each virtual machine has its own operating system inside it. Then the dependencies and then the application. This overhead causes higher utilization of underlying resources as there are multiple virtual operating systems and kernels running. The virtual machines also consume higher disk space as each VM is heavy and is usually in gigabytes in size, whereas Docker containers are lightweight and are usually in megabytes in size. This allows Docker containers to boot up faster, usually in a matter of seconds, whereas virtual machines, as we know, take minutes to boot up as it needs to boot up the entire operating system. It is also important to note that Docker has less isolation as more resources are shared between containers like the kernel, whereas VMs have complete isolation from each other. Since VMs don't rely on the underlying operating system or kernel, you can have different types of operating systems such as Linux-based or Windows-based on the same hypervisor, whereas it is not possible on a single Docker host. So these are some differences between the two. 

So how is it done? There are a lot of containerized versions of applications readily available as of today. So most organizations have their products containerized and available in a public Docker registry called **Docker Hub** or Docker Store already. For example, you can find images of most common operating systems, databases, and other services and tools. Once you identify the images you need and you install Docker on your host, bringing up an application stack is as easy as running a Docker run command with the name of the image. In this case, running a Docker run Ansible command will run an instance of Ansible on the Docker host. Similarly, run an instance of MongoDB, Redis, and Node.js using the Docker run command. When you run Node.js, just point to the location of the code repository on the host. If you need to run multiple instances of the web service, simply add as many instances as you need and configure a load balancer of some kind in the front. In case one of the instances was to fail, simply destroy that instance and launch a new instance. There are other solutions available for handling such cases that we will look at later during this course. 

We've been talking about images and containers. Let's understand the difference between the two. An image is a package or a template, just like a VM template that you might have worked with in the virtualization world. It is used to create one or more containers. Containers are running instances of images that are isolated and have their own environments and set of processes. As we have seen before, a lot of products have been Dockerized already. In case you cannot find what you're looking for, you could create an image yourself and push it to the Docker Hub repository, making it available for the public. 

If you look at it, traditionally, developers developed applications. Then they hand it over to the Ops Team to deploy and manage it in production environments. They do that by providing a set of instructions, such as information about how the host must be set up, what prerequisites are to be installed on the host, and how the dependencies are to be configured, etc. The Ops Team uses this guide to set up the application. Since the Ops Team did not develop the application on their own, they struggle with setting it up. When they hit an issue, they work with the developers to resolve it. With Docker, a major portion of work involved in setting up the infrastructure is now in the hands of the developers in the form of a Dockerfile. The guide that the developers built previously to set up the infrastructure can now easily be put together into a Dockerfile to create an image for the applications. This image can now run on any container platform and is guaranteed to run the same way everywhere. So the Ops Team now can simply use the image to deploy the application. Since the image was already working when the developer built it, and operations are not modifying it, it continues to work the same way when deployed in production. 


## Container Orchestration

Hello, and welcome to this lecture on container orchestration. So we learned about containers, and we now have our application packaged into a Docker container. But what's next? How do you run it in production? What if your application relies on other containers such as databases or messaging services or other backend services? What if the number of users increases and you need to scale your application? How do you scale down when the load decreases? To enable these functionalities, you need an underlying platform with a set of resources and capabilities. The platform needs to orchestrate the connectivity between the containers and automatically scale up or down based on the load. This whole process of automatically deploying and managing containers is known as container orchestration. 

Kubernetes is the container orchestration technology. There are multiple such technologies available today. Docker has its own tool called Docker Swarm. Kubernetes from Google and Mesos from Apache. While Docker Swarm is really easy to set up and get started, it lacks some of the advanced features required for complex applications. Mesos, on the other hand, is quite difficult to set up and get started but supports many advanced features. Kubernetes, arguably the most popular of them all, is a bit difficult to set up and get started but provides a lot of options to customize deployments and supports deployment of complex architectures. Kubernetes is now supported on all public cloud service providers like GCP, Azure and AWS. And the Kubernetes project is one of the top-ranked projects in GitHub. 

There are various advantages of container orchestration. Your application is now highly available as hardware failures do not bring your application down because you have multiple instances of your application running on different nodes. The user traffic is load balanced across the various containers. When demand increases, deploy more instances of the application seamlessly and within a matter of seconds. And we have the ability to do that at a service level. When we run out of hardware resources, scale the number of underlying nodes up or down without having to take down the application. And do all of these easily with a set of declarative object configuration files. And that is Kubernetes. It is a container orchestration technology used to orchestrate the deployment and management of hundreds and thousands of containers in a clustered environment.

## Kubernetes Architecture

Hello, and welcome to this lecture. Before we head into setting up a Kubernetes cluster, it is important to understand some of the basic concepts. This is to make sense of the terms that we will come across while setting up a Kubernetes cluster. Let us start with nodes. A node is a machine, physical or virtual, on which Kubernetes is installed. A node is a worker machine. And that is where containers will be launched by Kubernetes. It was also known as minions in the past. So you might hear these terms used interchangeably. But what if the node on which your application is running fails? Well, obviously, our application goes down. So you need to have more than one node. A cluster is a set of nodes grouped together. This way, even if one node fails, you have your application still accessible from the other nodes. Moreover, having multiple nodes helps in sharing load as well. Now we have a cluster, but who is responsible for managing the cluster? Where is the information about members of the cluster stored? How are the nodes monitored? When a node fails, how do you move the workload of the failed node to another worker node? That's where the master comes in. The master is another node with Kubernetes installed in it and is configured as a master. The master watches over the nodes in the cluster and is responsible for the actual orchestration of containers on the worker nodes. 

When you install Kubernetes on a system, you're actually installing the following components, an API server, an etcd service, a kubelet service, a container runtime, controllers and schedulers. The API server acts as the front end for Kubernetes. The users, management devices, command line interfaces, all talk to the API server to interact with the Kubernetes cluster. Next is the etcd keystore. etcd is a distributed reliable key value store used by Kubernetes to store all data used to manage the cluster. Think of it this way, when you have multiple nodes and multiple masters in your cluster, etcd stores all that information on all the nodes in the cluster in a distributed manner. etcd is responsible for implementing locks within the cluster to ensure that there are no conflicts between the masters. The scheduler is responsible for distributing work or containers across multiple nodes. It looks for newly created containers and assigns them to nodes. The controllers are the brain behind orchestration. They are responsible for noticing and responding when nodes, containers or endpoints go down. The controllers make decisions to bring up new containers in such cases. The container runtime is the underlying software that is used to run containers. In our case, it happens to be Docker. But there are other options as well. And finally, kubelet is the agent that runs on each node in the cluster. The agent is responsible for making sure that the containers are running on the nodes as expected. 

So far, we saw two types of servers, master and worker and a set of components that make up Kubernetes. But how are these components distributed across different types of servers? In other words, how does one server become a master and the other the worker? The worker node or minion as it is also known is where the containers are hosted. For example, Docker containers and to run Docker containers on a system, we need container runtime installed. And that's where the container runtime falls. In this case, it happens to be Docker. This doesn't have to be Docker. There are other container runtime alternatives available such as Rocket or Cri-o. But throughout this course, we are going to use Docker as our container runtime engine. The master server has the kube API server and that is what makes it a master. Similarly, the worker nodes have the Kubelet agent that is responsible for interacting with a master to provide health information of the worker node and carry out actions requested by the master on the worker nodes. All the information gathered are stored in a key-value store on the master. The key-value store is based on the popular etcd framework as we just discussed. The master also has the control manager and the scheduler. There are other components as well. But we will stop there for now. The reason we went through this is to understand what components constitute the master and worker nodes. This will help us install and configure the right components on different systems when we set up our infrastructure. 

And finally, we also need to learn a little bit about one of the command line utilities known as the kubectl command line tool or kubectl, or kube control as it is also called. The kubectl tool is used to deploy and manage applications on a Kubernetes cluster to get cluster information to get the status of other nodes in the cluster and to manage many other things. The kubectl run command is used to deploy an application on the cluster. The kubectl cluster info command is used to view information about the cluster. And the kubectl get nodes command is used to list all the nodes part of the cluster. That's all we need to know for now. And we will keep learning more commands throughout this course. We will explore more commands with kubectl when we learn the associated concepts. For now, just remember the run cluster info and get nodes commands.

## Runtime - CRI

Let's talk about CRI or **Container Runtime Interface**. In the beginning of the container era, Docker was the most popular container solution because it made working with containers super simple. Now, when Kubernetes was initially introduced, it only supported Docker container orchestration. However, with the growing popularity of Kubernetes, other container solutions like **Rocket** and **ContainerD** also wanted to integrate with Kubernetes. To address this, Kubernetes introduced the Container Runtime Interface or CRI. The CRI enables any container vendor to work as a container runtime for Kubernetes as long as they adhere to the **Open Container Initiative** or OCI standard. This means that Kubernetes can support different container runtimes without having to modify the core Kubernetes code. 

Now, the CRI is a plugin interface that defines the API for container runtimes to integrate with Kubernetes. 

So it defines the gRPC protocol that Kubernetes Kubelet uses to interact with container runtimes to manage container images and containers and networking. And CRI enables container runtimes to be developed independently of Kubernetes as long as they implement the CRI API. And so this allows for more flexibility in selecting the best container runtime for a specific use case as well as for innovation in the container runtime spec. 

Now, since Docker was already widely used with Kubernetes, Kubernetes had to continue to provide support for Docker even after introducing the CRI. And to do this, Kubernetes introduced a temporary solution called Docker Shim. So Docker Shim is a shim layer that enables Docker to communicate with Kubernetes without using the CRI and allowing Kubernetes to continue to manage Docker containers as before. However, Docker Shim is considered a temporary solution because it is only maintained for backwards compatibility with older versions of Kubernetes and Docker. And as Kubernetes evolved, it has become more container runtime agnostic and the use of Docker Shim is discouraged. So Docker Shim was removed from the Kubernetes version 1.24 and support for Docker was removed altogether. But remember, anything that worked with Docker will still work because it continues to work with ContainerD and ContainerD is an abstraction of Docker. Now, users are encouraged to use container runtimes that support the CRI and this allows for better compatibility and standardization across container runtimes and helps to avoid vendor lock-in. 

Now, after the removal of Docker support in Kubernetes, all Docker images continue to work seamlessly without any issues. So this is because Docker images adhere to the OCI or the Open Container Initiative standard, which allows them to be used with other container runtimes that also follow the OCI standards such as ContainerD. So that's kind of the story behind Container Runtime Interface and how it paved the path to increase compatibility and flexibility, enabling users to select the best container runtime of their choice and not really be dependent on a specific container runtime.

## Docker vs ContainerD

So you're going to come across Docker and ContainerD many times going forward. So when you read older blogs or documentation pages, you'll see Docker mentioned along with Kubernetes. And when you read newer blogs, you will see ContainerD. And you'll wonder what the difference is between the two. And there are a few CLI tools like CTR, CryoControl or NodeControl. And you'll wonder what are these CLI tools and which one should you be using? So that's what I'm going to explain in this video. So let's go back in time to the beginning of the Container era. And in the beginning, there was just Docker. And there were other tools like Rocket. But Docker's user experience made working with Containers super simple. And hence, Docker became the most dominant Container tool. And then came Kubernetes to orchestrate Docker. So Kubernetes was built to orchestrate Docker specifically in the beginning. So Docker and Kubernetes were tightly coupled. And back then, Kubernetes only worked with Docker and didn't support any other container solutions. And then Kubernetes grew in popularity as a Container Orchestrator. And now other Container runtimes like Rocket wanted in. So Kubernetes users needed it to work with container runtimes that are other than just Docker. And so Kubernetes introduced an interface called Container Runtime Interface or CRI. So CRI allowed any vendor to work as a Container runtime for Kubernetes as long as they adhere to the OCI standards. So OCI stands for Open Container Initiative. And it consists of an **image spec** and a **runtime spec**. Image spec means the specifications on how an image should be built. So that's what it defines. An image spec defines the specifications on how an image should be built. And the runtime spec defines the standards on how any container runtime should be developed. So keeping these standards in mind, anyone can build a Container runtime that can be used by anybody to work with Kubernetes. So that was the idea. So Rocket and other container runtimes that adhere to the OCI standards were now supported as container runtimes for Kubernetes via the CRI. However, Docker wasn't built to support the CRI standards. Because remember, Docker was built way before CRI was introduced and Docker still was the dominant container tool used by most. So Kubernetes had to continue to support Docker as well. And so Kubernetes introduced what is known as Docker Shim, which was a hacky but temporary way to continue to support Docker outside of the Container Runtime Interface. So while most other Container runtimes worked through the CRI, Docker continued to work without it. So now you see, Docker isn't just a container runtime alone. Docker consists of multiple tools that are put together. For example, the Docker CLI, the Docker API, the build tools that help in building images. There was support for volumes, auth, security, and finally, also the Container runtime called **runc** and the daemon that managed runc and that was called as **containerd**. So containerd is CRI compatible and can work directly with Kubernetes as all other runtimes. So ContainerD can be used as a runtime on its own, separate from Docker. So now you have ContainerD as a separate runtime and Docker separately. So Kubernetes continued to maintain support for Docker engine directly. However, having to maintain the Docker shim was an unnecessary effort and added complications. So it was decided in version 1.24 release of Kubernetes to remove the Docker shim completely. And so support for Docker was removed. But you see all the images that were built before Docker was removed. So all the Docker images continue to work because Docker followed the image spec from the OCI Open Standards. So all the images built by Docker follow the standard. So they continue to work with ContainerD but Docker itself was removed as a supported runtime from Kubernetes. So that's kind of the whole story. 

And now let's look into Containerd more specifically. So ContainerD, although it's part of Docker, is a separate project on its own now and is a member of CNCF with the graduated status. So you can now install ContainerD on its own without having to install Docker itself. So if you don't really need Docker's other features, you could ideally just install Containerd alone. So typically, we ran containers using the Docker run command when we had Docker. And if Docker isn't installed, then how do you run containers with just ContainerD? 

Now, once you install ContainerD, it comes with a command line tool called CTR. And this tool is solely made for debugging ContainerD and is not very user-friendly as it only supports a limited set of features. And this is what you can see in the documentation pages for this particular tool. So other than the limited set of features that it provides, any other way that you want to interact with Containerd, you'll have to rely on making API calls directly, which is not the most user-friendly way for us to operate. 

So just to give you an idea, the CRI command can be used to perform basic container-related activities such as pull images. For example, to pull a Redis image, you will run the ctr images pull command followed by the address of the image. And to run a container, we use the CTR run command and specify the image address. But as I mentioned, this tool is solely made for debugging ContainerD and is not very user-friendly and not to be used for running or managing containers on a production environment. 

So a better alternative recommended is the NerdControl tool or NerdCTL tool. So the NerdControl tool is a command line tool that's very similar to Docker. So it's like a Docker-like CLI for ContainerD. It kind of supports all or most of the options that Docker supports. And apart from that, it has the added benefit that it can give us access to the newest features implemented into ContainerD. So for example, we can work with the encrypted container images or other new features that will eventually be implemented into the regular Docker commands in the future. It also supports lazy pulling of images, P2P image distribution, image signing, and verifying and namespaces in Kubernetes, which is not available in Docker. 

So the NerdControl tool works very similarly to Docker CLI. So instead of Docker, you would ideally simply have to replace it with NerdControl. So you can run almost all Docker commands that interact with containers like this. So some examples are, instead of running the Docker run command to create a container, to run a container, you could just use the Nerdctl run command. And similarly, let's say you want to use some options for port mappings or exposing ports with the -p option for the Docker run command, you could do the same with kubectl, simply replace Docker with kubectl. So that's pretty easy and straightforward. 

So now that we have talked about CTR and the NerdControl tool, it's important to talk about another command line utility known as CRI CTL or CRI control. So earlier we talked about the Container Runtime Interface or CRI, which is a single interface used to connect CRI-compatible container runtimes like ContainerD, Rocket, and others. So the CRI control is a command line utility that is used to interact with the CRI-compatible container runtime. So this is kind of interaction from the Kubernetes perspective. So this tool is kind of maintained by, developed and maintained by the Kubernetes community and this tool works across all the different container runtimes. As opposed to earlier, you had the CTR and the NerdControl tool that were built by the ContainerD community specifically for ContainerD. This particular tool is from the Kubernetes perspective that works across different container runtimes. So it must be installed separately and it is used to inspect and debug container runtimes. So this again is not ideally used to create containers unlike Docker or the Kubernetes utility. This is again a debugging tool. You can technically create containers with the CRI control utility, but it's not easy. It's only to be used for some special debugging purposes. And remember that it kind of works along with the kubelet. So we know that the kubelet is responsible for ensuring that the specific number of containers or pods are available on a node at a time. So if you kind of go through the CRI control utility and try and create containers with it, then eventually kubelet is going to delete them because kubelet is unaware of some of those containers or pods that are created outside of its knowledge. So anything that it sees is going to go and delete it. So because of those things, remember that the CRI control utility is only used for debugging purposes and getting into containers and all of that. 

So let's look at some of the command line examples. So you simply run the kubectl command for this. And this can be used to perform basic container-related activities such as pull images or list existing images, list containers, very similar to the Docker command where you use the ps command. So in Docker, you will run the ps command. Here you run the kubectl get pods command. And to run a command inside a container, in Docker, remember we use the exact command and it's the same here, along with the same options such as the -i and -t. And then you specify the container ID and then the command that needs to be run. To view the logs, again, use the CRI control logs command, very similar to Docker command. And one major difference is that the CRI control command is also aware of pods. So you can list pods by running the kubectl get pods command. So this wasn't something that Docker was aware of. So while working with Kubernetes in the past, we used Docker commands a lot to troubleshoot containers and view logs, especially on the worker nodes. Now you're going to use the CRI control command to do so. So the syntax is a lot similar. So it shouldn't be really hard. 

So here's a chart that lists the comparison between Docker and the CRI control command line tool. So as you can see, a lot of commands such as attach, exec, images, info, inspect, logs, ps, stats, version, etc. work exactly the same way. And some of the commands to create, remove, and start and stop containers work similarly too. So a full list of differences can be found in the link given below. 

In the previous versions of Kubernetes, prior to version 1.24, the CRI control tool connected to runtime endpoints in the following default order as shown on screen. However, with the release of Kubernetes 1.24, a significant change was made. The docker-socket.socket points was replaced with cri-docker-d.sock. As a result, the updated default endpoints for the CRI control tool were changed. This change was implemented in response to the deprecation of the previous default settings. Now users are encouraged to manually set the endpoint. These changes are documented in the Kubernetes CRI tools repository, specifically in pull request and issue 868. It's recommended to review these discussions for a more detailed understanding of the changes. 

So to summarize, we have the CTR command line utility that comes with ContainerD and works with ContainerD, which is used for debugging purposes only and has a very limited set of features. So ideally, you wouldn't be using this at all. So you can kind of ignore this. Then we have the nerdctl, which is, again, from the containerd community, but this is a Docker-like CLI for containerd used for general purpose to create containers and supports the same or more features than docker-cli. So this is something that I think we'll be using a lot more going forward. And then we have the kubectl utility, which is from the Kubernetes community, mainly used to interact with CRI-compatible runtimes. So it's not just for ContainerD. This can be used for all CRI-supported runtimes. Again, this is mainly to be used for debugging purposes. So if you look at the comparisons here, you can see that CTR and kubectl are used mainly for debugging purposes, whereas the nerdctl is used for general purpose. The CTR and nerdctl are from the containerd community and works with containerd, whereas crictl is from the Kubernetes community and works across all CRI-compatible runtimes.