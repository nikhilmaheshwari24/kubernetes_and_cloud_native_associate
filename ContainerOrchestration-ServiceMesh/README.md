## Kubernetes Services

Let's briefly recap about Kubernetes services. So pods, as we know, are the smallest deployable units in Kubernetes. If you want to run something on Kubernetes, you need to create a pod and place the application in it. Pods are ephemeral, which means they can be created and killed. To match the cluster state you declare by Deployments. We need our Pods to find each other and communicate within the cluster so our application can work as expected. Pods, however, have their own IPs, which is good, but IPs keep changing because pods are well ephemeral. How do I find my application's pieces in a cluster and help them communicate like, let's say a front-end service talks to a back-end service, while these IPs are changing all the time. 

This is possible by using Kubernetes services. A back-end service can be created to point to the back-end pods so that any other application within the cluster can use the service to reach the applications in the back end. The service has its own IP and so we no longer have to rely on the IPs of the individual pods. Services are an abstraction that determines which pods to connect to and the policy on how to connect them. Pods have labels on them so that our service can select them in a huge pool of other pods. 

There are three types of services in Kubernetes: **NodePort**, **ClusterIP**, and **LoadBalancer**. The default type of service is called ClusterIP. This is an internal-only service that is created to enable communication between applications within the cluster. This type of service is only accessible within the cluster. If you would like to make an application accessible outside of the cluster, then you must create a service of type NodePort. This will make the service accessible on a predefined port on all the nodes in the cluster. The third type is called LoadBalancer. This type of service is only supported with specific cloud providers. This is like NodePort service but invokes a supported external load balancer to create a LoadBalancer to the ports the application exposed on the nodes in the clusters.

## Sidecars

In this section, we will talk about service meshes, and we will particularly talk about Istio, which is a popular service mesh, and we'll use that example to understand more about service meshes. But before we do that, let's first start with some prerequisites. During this section, we will be mentioning a lot about sidecars, so let's quickly brush up and understand what sidecars mean, right? So, you know the small passenger cabins attached to motorcycles? They are called Sidecars, right? And in the Kubernetes world, we know containers are encapsulated in Pods, and each Pod can have one or more containers. So the additional containers that support the main container are called sidecars, or sidecar containers. So they share the same volume, the same network, but they have different responsibilities. So sidecars are responsible for log shipping, or monitoring, file loading, or in our case, proxying, and they're relatively small in size. And our main container handles a business logic, a functionality that adds value to our application, which is the main code of the application, the main service that's run in the main container. And sidecars help in isolating these functionalities from our business logic. 

So here's a simple pod definition file for a sidecar container. So under the container section, first we have the nginx container, which is the main container that uses the nginx image. And then we have a second container created with a Fluentd image, which is our sidecar container that's responsible for shipping logs of the main application to a central server. So that's a sidecar container. And next we will talk about Envoy. 

```yaml
containers:
- name: nginx-container
  image: nginx
  volumeMounts:
  - name: shared-data
    mountPath: /usr/share/nginx/html
- name: sidecar-container
  image: fluent/fluentd
  volumeMounts:
  - name: shared-data
    mountPath: /pod-data
```

## Envoy

When you get into the service mesh world, Envoy is one of the most common proxies that you'll hear about. So let's have a look at it now. So Envoy is a proxy. But what is a proxy? Let's forget about containers and Kubernetes for a second and talk about a user trying to connect to an application. The application currently has its business logic, that is whatever the application does, and at the same time also does other things such as securing connections using TLS encryption or authentication or retrying requests if it is not able to serve the users etc. These will need to be coded into the application. What if all of these extra functionalities could be outsourced into a separate service so that the developers could focus solely on developing business logic, which is more fun. And that's called a proxy. 

Now the user reaches out to the proxy and the proxy forwards the request to the application. Envoy is an open-source proxy designed for modern service-oriented architectures. It was mainly started in the company **Lyft** in when they were trying to solve their own microservices problem. So it was mainly designed for distributed architectures and microservices. In 2017, the project was accepted to CNCF. In 2018, it reached the graduate level, which means it is battle-tested, ready for production and has a healthy number of contributors. 

Envoy is a proxy and communication bus with advanced capabilities and it works as a sidecar next to your container, just as we described in the last slide, so that the traffic going in and out of our pod is using Envoy as a proxy. Many service mesh implementations use Envoy now and we'll talk about Envoy more later in Istio's core architecture. 

## Monoliths & Microservices

When you are working on one large app, and if something breaks, it breaks entirely. However, if you are changing small portions of your app, things would be more under control. The piece that you are experimenting with might be affected, but the risk will be less than before. So we started designing our applications to be smaller and smaller so that we can isolate risks during experimentation, at the same time deploy faster and more frequently. So the giant systems we have been designing have become one big problem standing in the way of innovation and agility, and we had to take a look at our traditional software architectures and redesign them. 

So the term monolith is used when all the functionality in an app needs to be deployed at the same time, and there is a unified approach to all the separate functionalities within the monolithic boundaries. Now, all of this functionality always almost shares the same code base, and it has no clear boundaries between them, and these pieces are tightly coupled, so all the code could even be working as a single process. There is usually a single database for persistency in this model, which becomes a huge bottleneck at one point. 

Now, let's see how a monolithic application works in a real-life application. So here's a book info application, and we will use this as our example application throughout this course. It consists of four different modules, namely details, reviews, ratings, product page, and it is a modular application but still a monolith. Now, all the services depend on a specific version of the other one, so you need to deploy the whole package and possibly send some scripts to the database. Now, the product page shows the book's information, reviews, and ratings. All the data of this application comes from different modules, but they are not separately designed and cannot be scaled. So let's try to understand the dependencies of the modules and the problems of the monolithic book info application. Now, the customer lands on the product page. The product page gets this information from reviews and details modules. Reviews service gathers the number of ratings from the ratings service. They are all written in the same language, that's Java. And apart from these modules, the application also takes care of networking, authentication, authorization rules, and how data is transferred between modules, logging, monitoring, and tracing. And every once in a while, the ratings module, because of the amount of data it holds, has problems, and this affects the whole system. It's not possible to just scale ratings or leave it out of the system without touching the code and redeploying it. Now, a new campaign module is going to be developed, but this time a new team is formed for this and they want to use a different language. Since everything is unified and all the important functionality like authorization lies within the monolith, they have a hard time designing the architecture. And also our product owners want to try a new version of reviews with red stars for Christmas, for example. They want to test this functionality on a segment of users, and if they like it, we can use it for the whole system. 

Now, this is not possible without deploying the entire application in two different versions. This is a very simple app with very little functionality, and you can see the problems we are having to tackle already. Now, think of huge applications that have been around for decades and more. There may be hundreds or more developers working on these systems and starting with some loose rules on architecture, and these systems might become a big ball of mud. Now, a big ball of mud is a famous idiom for these types of software systems. Without noticing, your monolithic application can go out of hand and become one of these. 

Now, let's have a look at how this Bookinfo monolithic application could magically and miraculously turn into microservices. Well, to be honest, it's not an easy task to refactor monoliths, and it's not an overnight transformation. It's a cultural, technical, and organizational effort which paves the way to being CloudNative. And with the new Microservices architecture, each module is now its own independent and separate application. Our product page has been transformed into a Python app. It still functions as our landing page. The book details module has been refactored into a Ruby application. The reviews module has been transformed into a Java app. The ratings module has now been redesigned and implemented into Node.js. Moreover, the reviews app now has multiple versions, v1, v2, and v3, to test different ideas. v1 is a no-star version, v2 is a black star version, and v3 is a red star version. 

Now, as before, users land on the product page, which contains the details and reviews services to show information regarding various products. So now that we have seen how the monolithic Bookinfo application has been transformed into microservices, let's talk about the improvements and wins. Now, the ratings module will not be a problem anymore. Now that it is fully independent, we can scale it up or down depending on the load from our customers. We can now deploy every piece of Bookinfo without interfering with the others. It will make our releases smaller, faster, and less risky. With microservices architecture, each service can be written with different languages. So here in our application, we have now four different languages, and each team has autonomy. Our services now are isolated from the failure of other services because of the loose coupling. And the end-to-end application will have more resilience since its different parts can be monitored, changed, or rolled back easily. Instead of having one big application, we now have six smaller applications. So hopefully you won't have to deal with a big ball of mud anymore. In an ideal scenario, a microservice should have a single responsibility. 

Earlier when we discussed the monolithic Book application, we said that apart from the four different modules, the application also takes care of networking, authentication, authorization rules, and how data is transferred between modules, as well as logging and monitoring. But what happened to these when we moved to the microservices model? Well, they're not there. Our microservices currently do not implement any of these. We could move these into each of our services. So let's add all of these functionalities into our Microservices. And by now, you probably have an idea on where this is going. Every microservice boundary has the same functionality coded again and again into them. Every team has to solve the same issues over and over again, and probably they'll solve them differently. Look at the code duplication here, and how do you go tell these teams to change a certificate or a monitoring agent version? Any developer developing any of these applications will need to be aware of all these extra components, apart from the core business logic that microservices are meant to serve. These issues we have to deal with in every microservices are called cross-cutting concerns. When coded into the microservice, they disrupt the main reason we design microservices, to be able to have smaller, more independent pieces. This is known as the problem of fat microservices. So microservices are not a piece of cake. They have their own challenges, and they tend to get really complicated. 

And as we just saw in the monolithic version of our application, all aspects such as networking and security were directly coded into the application. But now all of those gray areas fabricated into our monolithic application are exposed, and we have to find a way to cover them. And how will the product page know which version of reviews to go to? So how will one service know how to find the other one? And what are the traffic rules? What are the timeouts? In a very short time, you'll have too many tiny bits of services spread out, and these questions will get much harder to answer. It was much easier to handle the security in our monolith, because it provided the proper schema for us in our microservices, securing service-to-service communication and end-to-end user-to-service communication, and has become a problem itself. And now that you have loosely coupled tiny pieces and many abstraction layers, it gets harder to pinpoint a problem in your application. And for that, you will need an observability strategy. And just for a small application, we used four different languages with all those different technologies of Microservices. It gets very demanding to do traditional operations. And actually, operations might become a bottleneck for organizations who take on microservices. There is a new approach for that called DevOps, where the development teams work closely with operations, and together they take the responsibility of their microservices for deploying, monitoring, and fixing. In the upcoming lesson, we will look at how service meshes can help solve these challenges with microservices. 

## Service Mesh

So what is a service mesh? This is where we were in the previous video. Instead of embedding all the different requirements into each microservice, we replace them with a single proxy in the form of a sidecar container. The proxies communicate with each other through what is known as a data plane and they communicate to a server-side component called control plane. Now the control plane manages all the traffic into and out of your services via proxy. So all networking logic is abstracted from your business code and this approach is known as service mesh. A service mesh is a dedicated and configurable infrastructure layer that handles the communication between services without having to change the code in a microservice architecture. 

With a service mesh, you can dynamically configure how services talk to each other. When services talk to one another, you'll have mutual TLS so your workloads can be secure. You can see things better, for example, how the application is doing end-to-end, where it is having issues and bottlenecks, and service discovery, which covers three main topics. In a dynamic cluster, we will need to know at which IP and port services are exposed so that they can find each other. Health checks help you dynamically keep services that are up in the mesh while services that are down are left out. Load balancing routes the traffic to healthy instances and cuts it off from the ones who have been failing and we will look into each of these in more detail throughout the rest of the videos in this section. 

## Istio

Now, we'll have a look at Istio, how it works, its architecture and its components. Istio is a free and open-source service mesh that provides an efficient way to secure, connect and monitor services. Istio works with Kubernetes and traditional workloads, thereby bringing universal traffic management, telemetry, and security to complex deployments. Istio is supported and implemented by leading CloudNative providers and consultants. Earlier, we talked about the proxy service that takes care of all the tasks that should be outsourced from the microservice. These proxies and the communication between them form the data plane. Istio implements these proxies using an open-source high-performance proxy known as Envoy. And the proxies talk to a server-side component known as the control plane. And so let's take a look at these in a bit more detail. Originally, the control plane consisted of three components named Citadel, Pilot, and Galley. Citadel managed certificate generation, Pilot helped with service discovery and Galley helped in validating configuration files. The three components were later combined into a single daemon called Istio-Discovery. Each service or pod has a separate component in it along with the Envoy proxy called the Istio agent. The Istio agent is responsible for passing configuration secrets to the Envoy proxies. So that's a high-level overview of Istio. 

## Installing Istio

Now it's time to install Istio on our cluster. So there are three different approaches to install Istio. We could install it using the command line utility, Istioctl. We could use an Istio operator to install it, or we could use a Helm package for the same installation. So for this training, we'll use Istioctl for installing Istio on our cluster. To install Istio using the Istioctl command, run the Istioctl install command and specify a profile. For this installation, we'll use a demo profile. There are also different profiles for production and performance testing. We'll not use them in our demo, but it's important to know that there are different environments that need different profiles. When this command is run, Istio is deployed in the cluster in the form of a deployment named Istio-operator. In a new namespace known as the Istio system namespace, Istio-operator, as we discussed earlier, has the different components inside it, such as Citadel, Pilot, and Galley. Along with these, it also deploys two other services known as the Istio egress gateway and Istio ingress gateway, and a bunch of Kubernetes service objects to expose these services within the cluster. We'll talk more about these gateways when we talk about gateways in a later section. Now once installed, run the istioctl verify install command to verify the installation. You'll see a huge list of items here, as we already talked about it. Istio extends Kubernetes, so you can see CRDs here. And let's now see this in action in the upcoming demo. 

## Deploying our First Application on Istio

This is for deploying our first application on Istio. Now to try Istio, let's deploy our application. In the samples folder we downloaded, you will find the sample Bookinfo application. So deploy it, use the command. If you have Istio samples somewhere else on your computer, you can also change this directory. In the output that follows, you will see a number of Deployments and Services created. So let's check the status of the Pods deployed. We see that we have a Pod for the product page Microservice. So the details microservices, the rating service, and the three different versions of the review service. We see that everything is deployed in the default namespace. And now that we have Istio installed, we expect each pod to have the additional proxy container that we talked about. However, that is not to be seen. So we can see that each pod only has a single container as shown in the ready column here. So why is that? Now we can use the Istio CTL analyze command to see why that may be the case. It seems that we have a problem here. The analysis tells us that Istio injection is not enabled. So what does that mean? What does it mean by namespace is not enabled for Istio injection? Now you might be having multiple namespaces on your Kubernetes cluster. So kube-system being the namespace where all the core applications run and the default being the default namespace where applications are deployed when no namespace is specifically given. As it is in our case, there may be other applications running in other namespaces such as HR or Payroll, etc. You must explicitly enable Istio sidecar injection at a namespace level. If you would like Istio to inject proxy services as sidecars to the applications deployed in namespaces. For this, as given in the output of the command, you must run the kubectl label command to specify the namespace where you want to enable sidecar injection by setting the value of the label Istio injection to enable. Similarly, if you'd like to explicitly disable Istio injection, then you must set this label to disabled using the same command. We will now delete what we deployed so that we can set the labels and deploy it again. Let's enable Istio sidecar injection in the default namespace. And once this command is run, every new app in the default namespace will get a sidecar automatically. Now let's deploy our app again. Let's check the status of our mesh now. And let's check if we have any sidecar proxies. And yes, they're there and it's running. So Istio has now injected sidecar proxies into each Pod. We have now successfully set up Istio on our Cluster. 

## Continue Learing Istio

Well, so that was a quick introduction to what service meshes are and what Istio is and how to deploy Istio on your Kubernetes cluster. In the scope of this particular course, this should be sufficient. But if you'd like to learn more about Istio specifically, check out the course on Istio on the KodeKloud platform. In this course, you'll learn Istio by doing with a lot of hands-on labs, and you'll learn concepts in much more depth. Well, thanks for watching, and I'll see you in the next one. 