## Pod

Hello, and welcome to this lecture on Kubernetes pods. Before we head into understanding pods, we would like to assume that the following have been set up already. At this point, we assume that the application is already developed and built into Docker images, and it is available on a Docker repository, like Docker Hub, so Kubernetes can pull it down. We also assume that the Kubernetes cluster has already been set up and is working. This could be a single node setup or a multi-node setup, doesn't matter. All the services need to be in a running state. 

As we discussed before, with Kubernetes, our ultimate aim is to deploy our application in the form of containers on a set of machines that are configured as worker nodes in a cluster. However, Kubernetes does not deploy containers directly on the worker nodes. The containers are encapsulated into a Kubernetes object known as pods. A pod is a single instance of an application. A pod is the smallest object that you can create in Kubernetes. 

Here we see the simplest of simplest cases where you have a single node Kubernetes cluster with a single instance of your application running in a single Docker container encapsulated in a pod. What if the number of users accessing your application increases, and you need to scale your application, you need to add additional instances of your web application to share the load. Now, where would you spin up additional instances? Do we bring up new container instances within the same pod? No, we create a new pod altogether with a new instance of the same application. As you can see, we now have two instances of our web application running on two separate pods on the same Kubernetes system or node. What if the user base further increases and your current node has no sufficient capacity? Well, then you can always deploy additional pods on a new node in the cluster, you will have a new node added to the cluster to expand the cluster's physical capacity. So what I'm trying to illustrate in this slide is that pods usually have a one-to-one relationship with containers running your application. To scale up, you create new pods. And to scale down, you delete an existing pod; you do not add additional containers to an existing pod to scale your application. Also, if you're wondering how we implement all of this, and how we achieve load balancing between the containers, etc., we will get into all of that in a later lecture. For now, we are only trying to understand the basic concepts. 

We just said that pods usually have a one-to-one relationship with the containers. But are we restricted to having a single container in a single pod? No, a single pod can have multiple containers, except for the fact that there are usually not multiple containers of the same kind. As we discussed in the previous slide, if our intention was to scale our application, then we would need to create additional pods. But sometimes you might have a scenario where you have a helper container that might be doing some kind of supporting task for our web application, such as processing a user entered data, processing a file uploaded by the user, etc. And you want these helper containers to live alongside your application container. In that case, you can have both of these containers part of the same pod. So that when a new application container is created, the helper is also created. And when it dies, the helper also dies since they are part of the same pod. The two containers can also communicate with each other directly by referring to each other as local hosts, since they share the same network space. Plus they can easily share the same storage space as well. 

If you still have doubts on this topic, I would understand if you did because I did the first time I learned these concepts. We could take another shot at understanding pods from a different angle. Let's for a moment keep Kubernetes out of our discussion and talk about simple Docker containers. Let's assume we were developing a process or a script to deploy our application on a Docker host. Then we would first simply deploy our application using a simple Docker run Python app command, and the application runs fine and our users are able to access it. When the load increases, we deploy more instances of our application by running the Docker run commands many more times. This works fine and we're all happy. Now sometime in the future, our application is further developed, undergoes architectural changes, and grows and gets complex. We now have a new helper container that helps our web application by processing or fetching data from elsewhere. These helper containers maintain a one-to-one relationship with our application container and thus need to communicate with the application containers directly and access data from those containers. For this, we need to maintain a map of what app and helper containers are connected to each other; we would need to establish network connectivity between these containers ourselves. Using links and custom networks, we would need to create shareable volumes and share them among the containers; we would need to maintain a map of that as well. And most importantly, we would need to monitor the state of the application container. And when it dies, manually kill the helper container as well as it's no longer required. When a new container is deployed, we would need to deploy the new helper container as well. With pods, Kubernetes does all of this for us automatically, we just need to define what containers a pod consists of. And the containers in a pod by default will have access to the same storage, the same network namespace, and same fate, as in they will be created together and destroyed together. Even if our application didn't happen to be so complex, and we could live with a single container, Kubernetes still requires you to create pods. But this is good in the long run as your application is now equipped for architectural changes and scale in the future. However, also note that multi-pod containers are a rare use case. And we're going to stick to single containers per pod in this course. 

This is now look at how to deploy pods. Earlier, we learned about the kubectl run command. What this command really does is it deploys a Docker container by creating a pod. So it first creates a pod automatically and deploys an instance of the nginx Docker image. But where does it get the application image from? For that, you need to specify the image name using the image parameter. The application image in this case, the nginx image is downloaded from the Docker Hub repository. Docker Hub, as we discussed, is a public repository where latest Docker images of various applications are stored. You could configure Kubernetes to pull the image from the public Docker Hub or a private repository within the organization. Now that we have a pod created, how do we see the list of pods available? The kubectl get pods command helps us see the list of pods in our cluster. In this case, we see the pod is in a container creating state, and soon changes to a running state when it is actually running. Also remember that we haven't really talked about the concepts on how a user can access the nginx web server. And so in the current state, we haven't made the web server accessible to external users. You can access it internally from the node. But for now, we will just see how to deploy a pod. And later in a later lecture, once we learn about networking and services, we will get to know how to make this service accessible to end users. 

```bash
$ kubectl run nginx --image nginx
$ kubectl get pods
```

## Pods - with YAML

Hello, and welcome to this lecture. In this lecture, we will talk about creating a pod using a YAML based configuration file. In the previous lecture, we learned about YAML files in general. Now we will learn how to develop YAML files specifically for Kubernetes. Kubernetes uses YAML files as inputs for the creation of objects such as Pods, ReplicaSets, Deployments, Services, etc. All of these follow a similar structure. A Kubernetes definition file always contains four top level fields, the API version, kind, metadata, and spec. These are the top level or root level properties. These are also required fields, so you must have them in your configuration file. Let us look at each one of them. 

The first one is the **API version**. This is the version of the Kubernetes API we are using to create the object. Depending on what we are trying to create, we must use the right API version. For now, since we are working on pods, we will set the API version as v1. Few other possible values for this field are apps/v1, beta extensions/v1 beta, etc. We will see what these are for later in this course. 

Next is the **kind**. The kind refers to the type of object we are trying to create, which in this case happens to be a pod. So we will set it as pod. Some other possible values here could be ReplicaSet or Deployment or Service, which is what you see in the kind field in the table on the right. 

The next is **metadata**. The metadata is data about the object like its name, labels, etc. As you can see, unlike the first two where you have specified a string value, this is in the form of a dictionary. So everything under metadata is indented to the right a little bit. And so names and labels are children of metadata. The number of spaces before the two properties name and labels doesn't matter. But they should be the same as they are siblings. In this case, labels has more spaces on the left than name. And so it is now a child of the name property instead of a sibling, which is incorrect. Also, the two properties must have more spaces than their parent, which is metadata, so that it's indented to the right a little bit. In this case, all three of them have the same number of spaces before them. And so they are all siblings, which is not correct. Under metadata, the name is a string value. So you can name your pod, my app pod. And the labels are a dictionary. So labels are a dictionary within the metadata dictionary. And it can have any key and value pairs as you wish. For now, I have added a label app with the value my app. Similarly, you could add other labels as you see fit, which will help you identify these objects at a later point in time. Say for example, there are hundreds of Pods running a front end application and hundreds of Pods running a back end application or a database, it will be difficult for you to group these Pods once they are deployed. If you label them now as front end, back end or database, you will be able to filter the Pods based on this label at a later point in time. It's important to note that under metadata, you can only specify name or labels or anything else that Kubernetes expects to be under metadata, you cannot add any other property as you wish under this. However, under labels, you can have any kind of key or value pairs as you see fit. So it's important to understand what each of these parameters expect. So far, we have only mentioned the type and name of the object we need to create, which happens to be a Pod with the name my app Pod. But we haven't really specified the container or image we need in the Pod. 

The last section in the configuration file is the specification section, which is written as **spec**. Depending on the object we are going to create, this is where we would provide additional information to Kubernetes pertaining to that object. This is going to be different for different objects. So it's important to understand or refer to the documentation section to get the right format for each. Since we are only creating a pod with a single container in it, it is easy. Spec is a dictionary, so add a property under it called containers. Containers is a list or an array. The reason this property is a list is because the pods can have multiple containers within them, as we learned in the lecture earlier. In this case, though, we will only add a single item in the list, since we plan to have only a single container in the pod. The dash right before the name indicates that this is the first item in the list. The item in the list is a dictionary. So add a name and image property, the value for image is nginx, which is the name of the Docker image in the Docker repository. Once the file is created, run the command kubectl create -f followed by the file name, which is pods-definition.yaml and Kubernetes creates the pod. 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec: 
  containers:
  - name: nginx-container
    image: nginx  
```

So to summarize, remember the four top level properties, API version, kind, metadata, and spec, then start by adding values to those depending on the object you are going to create. Once we create the pod, how do you see it? Use the kubectl get pods command to see a list of pods available. In this case, it's just one. To see detailed information about the pod, run the kubectl describe pod command. This will tell you information about the pod, when it was created, what labels are assigned to it, what Docker containers are part of it, and the events associated with that pod. 

```bash
$ kubectl create -f pod.yaml
$ kubectl get pods
$ kubectl describe nginx
```

## Replicaset

So what is a replica? And why do we need a replication controller? Let's go back to our first scenario where we had a single pod running our application. What if for some reason our application crashes and the pod fails, users will no longer be able to access our application. To prevent users from losing access to our application, we would like to have more than one instance or pod running at the same time. That way, if one fails, we still have our application running on the other one. The replication controller helps us run multiple instances of a single pod in the Kubernetes cluster, thus providing high availability. So does that mean you can't use a replication controller if you plan to have a single pod? No, even if you have a single pod, the replication controller can help by automatically bringing up a new pod when the existing one fails. Thus, the replication controller ensures that the specified number of pods are running at all times, even if it's just one or a hundred. Another reason we need the replication controller is to create multiple pods to share the load across them. For example, in this simple scenario, we have a single pod serving a set of users. When the number of users increases, we deploy additional pods to balance the load across the two pods. If the demand further increases, and if we were to run out of resources on the first node, we could deploy additional pods across the other nodes in the cluster. As you can see, the replication controller spans across multiple nodes in the cluster. It helps us balance the load across multiple pods on different nodes as well as scale our application when the demand increases. 

It's important to note that there are two similar terms, replication controller and replica set. Both have the same purpose, but they're not the same. Replication controller is the older technology that is being replaced by ReplicaSet. Replica Set is the new recommended way to set up replication. However, whatever we discussed in the previous few slides remains applicable to both these technologies. There are minor differences in the way each works, and we will look at that in a bit. As such, we will try to stick to ReplicaSets in all of our demos and implementations going forward. Let us now look at how we create a ReplicationController. As with the previous lecture, we start by creating a ReplicationController definition file. We will name it rc-definition.yaml. As with any Kubernetes definition file, we have four sections, the API version, kind, metadata, and spec. The API version is specific to what we are creating. In this case, ReplicationController is supported in Kubernetes API version v1. So we will set it as v1. The kind, as we know, is Replication Controller. Under metadata, we will add a name, and we will call it my-app-RC. And we will also add a few labels, app and type, and assign values to them. So far, it has been very similar to how we created a Pod in the previous section. The next is the most crucial part of the definition and that is the specification written as spec. For any Kubernetes definition file, the spec section defines what's inside the object we are creating. In this case, we know that the ReplicationController creates multiple instances of a Pod. But what Pod? We create a template section under spec to provide a Pod template to be used by the Replication Controller to create replicas. Now, how do we define the Pod template? It's not that hard, because we have already done that in the previous exercise. Remember, we created a Pod definition file in the previous exercise, we could reuse the contents of the file to populate the template section. Move all the contents of the Pod definition file into the template section of the ReplicationController, except for the first few lines, which are API version and kind. Remember, whatever we move must be under the template section, meaning they should be indented to the right and have more spaces before them than the template line itself. They should be children of the template section. Looking at our file now, we now have two metadata sections. One is for the ReplicaSet and another for the pod. And we have two aspect sections, one for each. We have nested two definition files together, the ReplicaSet being the parent and the pod definition being the child. Now there is something still missing. We haven't mentioned how many replicas we need in the ReplicaSet. For that, add another property to the spec called replicas and input the number of replicas you need under it. Remember that the template and replicas are direct children of spec sections. So they are siblings and must be on the same vertical line, which means having an equal number of spaces before them. Once the file is ready, run the kubectl create command and input the file using the -f parameter. The replication controller is created. When the replication controller is created, it first creates the pod using the pod definition template as many as required, which is three in this case. To view the list of created replication controllers run the kubectl get replication controller command and you will see the replication controller listed. We can also see the desired number of replicas or pods, the current number of replicas and how many of them are ready in the output. If you would like to see the pods that were created by the replication controller, run the kubectl get pods command and you will see three pods running. Note that all of them are starting with the name of the replication controller, which is myapp-rc, indicating that they are all created automatically by the replication controller. What we just saw was the replication controller. Let us now look at replica set. It is very similar to replication controller. As usual, first we have API version, kind, metadata and spec. The API version though is a bit different. It is apps/v1, which is different from what we had before for replication controller, which was just v1. If you get this wrong, you're likely to get an error that looks like this. It would say no match for kind ReplicaSet because the specified Kubernetes API version has no support for ReplicaSet. The kind would be ReplicaSet. And we add name and labels in the metadata. The specification section looks very similar to replication controller. It has a template section where we provide pod definition as before. So I'm going to copy contents over from pod definition file and we have number of replicas which is set to three. 

However, there is one major difference between replication controller and ReplicaSet. ReplicaSet requires a selector definition. The selector section helps the ReplicaSet identify what pods fall under it. But why would you have to specify what parts fall under it if you have provided the contents of the pod definition file itself in the template. It's because replica set can also manage pods that were not created as part of the replica set creation. Say for example, there were pods created before the creation of the replica set that match labels specified in the selector. The replica set will also take those pods into consideration when creating the replicas. I will elaborate on this in the next slide. But before we get into that, I would like to mention that the selector is one of the major differences between replication controller and replica set. The selector is not a required field in the case of a replication controller, but it is still available. When you skip it, as we did in the previous slide, it assumes it to be the same as the labels provided in the pod definition file. In the case of replica set, user input is required for this property, and it has to be written in the form of match labels. As shown here, the match labels selector simply matches the labels specified under it to the labels on the pod. The replica set selector also provides many other options for matching labels that were not available in a replication controller. And as always, to create a replica set run the kubectl create command providing the definition file as input. And to see the created replicas run the kubectl get replica set command to get a list of pods simply run the kubectl get pods command. So what is the deal with labels and selectors? Why do we label our pods and objects in Kubernetes? Let us look at a simple scenario. Say we deployed three instances of our front end web application as three pods, we would like to create a replication controller or replica set to ensure that we have three active pods at any time. And yes, that is one of the use cases of replica sets, you can use it to monitor existing pods if you have them already created as it is in this example. In case they were not created, the ReplicaSet will create them for you. The role of the ReplicaSet is to monitor the Pods and if any of them were to fail, deploy new ones. The ReplicaSet is in fact a process that monitors the Pods. Now how does the ReplicaSet know what Pods to monitor? There could be hundreds of other pods in the cluster running different applications. This is where labeling our pods during creation comes in handy. We could now provide these labels as a filter for Replica Sets. Under the selector section, we use the match labels filter and provide the same label that we used while creating the pods. This way, the Replica Set knows which pods to monitor. The same concept of labels and selectors is used in many other places throughout Kubernetes. Now let me ask you a question along the same lines. In the ReplicaSet specification section, we learned that there are three sections: template, replicas, and the selector. We need three replicas and we have updated our selector based on our discussion in the previous slide. Say for instance, we have the same scenario as in the previous slide where we have three existing Pods that were created already. And we need to create a ReplicaSet to monitor the Pods to ensure there are a minimum of three running at all times. When the replication controller is created, it is not going to deploy a new instance of Pod as three of them with matching labels are already created. In that case, do we really need to provide a template section in the ReplicaSet specification? Since we are not expecting the ReplicaSet to create a new Pod on deployment? Yes, we do. Because in case one of the Pods were to fail in the future, the Replica Set needs to create a new one to maintain the desired number of Pods. And for the Replica Set to create a new Pod, the template definition section is required. Let's now look at how we scale the Replica Set. Say we started with three replicas, and in the future we decided to scale to six. How do we update our ReplicaSet to scale to six replicas? Well, there are multiple ways to do it. The first is to update the number of replicas in the definition file to six. Then run the kubectl replace command to specify the same file using the -f parameter and that will update the ReplicaSet to have six replicas. The second way to do it is to run the kubectl scale command, use the replicas parameter to provide the new number of replicas and specify the same file as input. You may either input the definition file or provide the ReplicaSet name in the type name format. However, remember that using the file name as input will not result in the number of replicas being updated automatically in the file. In other words, the number of replicas in the ReplicaSet definition file will still be three, even though you scaled your ReplicaSet to have six replicas using the kubectl scale command and the file as input. There are also options available for automatically scaling the ReplicaSet based on load. But that is an advanced topic, and we will discuss it at a later time. Let us now review the commands real quick. The kubectl create command as we know is used to create a ReplicaSet, or basically any object in Kubernetes, depending on the file we are providing as input. You must provide the input file using the -f parameter. Use the kubectl get command to see a list of ReplicaSets created. Use the kubectl delete ReplicaSet command followed by the name of the ReplicaSet to delete the ReplicaSet. And then we have the kubectl replace command to replace or update the ReplicaSet. And also the kubectl scale command to scale ReplicaSet simply from the command line without having to modify the file. 
